{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9537222,"sourceType":"datasetVersion","datasetId":5809117},{"sourceId":9608917,"sourceType":"datasetVersion","datasetId":5863008}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(\"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\", add_to_git_credential=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\ndef analyze_layers_in_range(model, lower_bound, upper_bound):\n    layers_in_range = []\n    layers_out_of_range = []\n    all_layers = []  # To keep track of the order in which layers are encountered\n\n    for name, param in tqdm(model.named_parameters(), desc=\"Analyzing parameters\"):\n        if param.requires_grad:\n            param_cpu = param.detach().cpu().float()\n            \n            # Check if all parameters in the layer are within the specified range\n            in_range = ((param_cpu >= lower_bound) & (param_cpu <= upper_bound)).all().item()\n            \n            if in_range:\n                layers_in_range.append(name)\n            else:\n                layers_out_of_range.append(name)\n            \n            # Record the order of layers\n            all_layers.append((name, in_range))\n\n    return layers_in_range, layers_out_of_range, all_layers\n\ndef print_layer_inlier_outlier(all_layers):\n    print(\"\\nLayer Status Report:\")\n    print(\"=\" * 40)\n    \n    # Print information for each layer in the order they were encountered\n    for layer_name, in_range in all_layers:\n        if in_range:\n            print(f\"{layer_name}: Inlier (All parameters within range)\")\n        else:\n            print(f\"{layer_name}: Outlier (Some parameters out of range)\")\n\ndef plot_layer_status(all_layers, output_path):\n    # Extract statuses\n    statuses = [1 if in_range else 0 for _, in_range in all_layers]\n\n    plt.figure(figsize=(20, 10))  # Adjusted figure size for better readability\n    bar_width = 0.8  # Set a fixed width for the bars\n    x = range(len(all_layers))  # X-axis positions for the bars\n\n    # Plot the bars\n    plt.bar(x, [1] * len(all_layers), color=['green' if status == 1 else 'red' for status in statuses], width=bar_width)\n\n    # Set the x-ticks to be numbered (0, 1, 2, ..., len(all_layers)-1)\n    plt.xticks(x[::10], [i for i in range(len(all_layers))][::10], rotation=90)  # Show every 10th tick for clarity\n\n    # Label the axes\n    plt.xlabel('Layer Index')\n    plt.ylabel('Status')\n    plt.title('Layer Parameter Status')\n\n    # Remove everything under x-axis\n    plt.gca().spines['bottom'].set_visible(False)\n    plt.gca().spines['left'].set_visible(False)\n    plt.gca().xaxis.set_ticks_position('none')\n    plt.gca().yaxis.set_ticks_position('none')\n\n    # Adjust layout to make room for labels\n    plt.tight_layout()\n\n    # Save the plot\n    plt.savefig(output_path, bbox_inches='tight')  # Save the plot to the specified file path\n    plt.close()  # Close the figure to free up memory\n\n# Load model and tokenizer\nmodel_name = \"stabilityai/japanese-stablelm-base-beta-7b\"  # Update with the correct model name\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n\nlower_bound, upper_bound = -1, 1\n\nlayers_in_range, layers_out_of_range, all_layers = analyze_layers_in_range(model, lower_bound, upper_bound)\n\nprint(f\"Layers with all parameters in range: {len(layers_in_range)}\")\nprint(f\"Layers with any parameters out of range: {len(layers_out_of_range)}\")\n\n# Print the layer status report in the order encountered\nprint_layer_inlier_outlier(all_layers)\n\n# Define the output path (adjust this path to a location on your local system)\noutput_path = \"layer_status_plot_stablelm.png\"  # Path to save the plot\n\n# Plot the layer status bar graph and save it\nplot_layer_status(all_layers, output_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\ndef analyze_layers_in_range(model, lower_bound, upper_bound):\n    layers_in_range = []\n    layers_out_of_range = []\n    all_layers = []  # To keep track of the order in which layers are encountered\n\n    for name, param in tqdm(model.named_parameters(), desc=\"Analyzing parameters\"):\n        if param.requires_grad:\n            param_cpu = param.detach().cpu().float()\n            \n            # Check if all parameters in the layer are within the specified range\n            in_range = ((param_cpu >= lower_bound) & (param_cpu <= upper_bound)).all().item()\n            \n            if in_range:\n                layers_in_range.append(name)\n            else:\n                layers_out_of_range.append(name)\n            \n            # Record the order of layers\n            all_layers.append((name, in_range))\n\n    return layers_in_range, layers_out_of_range, all_layers\n\ndef print_layer_inlier_outlier(all_layers):\n    print(\"\\nLayer Status Report:\")\n    print(\"=\" * 40)\n    \n    # Print information for each layer in the order they were encountered\n    for layer_name, in_range in all_layers:\n        if in_range:\n            print(f\"{layer_name}: Inlier (All parameters within range)\")\n        else:\n            print(f\"{layer_name}: Outlier (Some parameters out of range)\")\n\ndef plot_layer_status(all_layers, output_path):\n    # Extract statuses\n    statuses = [1 if in_range else 0 for _, in_range in all_layers]\n\n    plt.figure(figsize=(20, 10))  # Adjusted figure size for better readability\n    bar_width = 0.8  # Set a fixed width for the bars\n    x = range(len(all_layers))  # X-axis positions for the bars\n\n    # Plot the bars\n    plt.bar(x, [1] * len(all_layers), color=['green' if status == 1 else 'red' for status in statuses], width=bar_width)\n\n    # Set the x-ticks to be numbered (0, 1, 2, ..., len(all_layers)-1)\n    plt.xticks(x[::10], [i for i in range(len(all_layers))][::10], rotation=90)  # Show every 10th tick for clarity\n\n    # Label the axes\n    plt.xlabel('Layer Index')\n    plt.ylabel('Status')\n    plt.title('Layer Parameter Status')\n\n    # Remove everything under x-axis\n    plt.gca().spines['bottom'].set_visible(False)\n    plt.gca().spines['left'].set_visible(False)\n    plt.gca().xaxis.set_ticks_position('none')\n    plt.gca().yaxis.set_ticks_position('none')\n\n    # Adjust layout to make room for labels\n    plt.tight_layout()\n\n    # Save the plot\n    plt.savefig(output_path, bbox_inches='tight')  # Save the plot to the specified file path\n    plt.close()  # Close the figure to free up memory\n\n# Load model and tokenizer\nmodel_name = \"openbmb/MiniCPM-V-2\"  # Update with the correct model name\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n\nlower_bound, upper_bound = -1, 1\n\nlayers_in_range, layers_out_of_range, all_layers = analyze_layers_in_range(model, lower_bound, upper_bound)\n\nprint(f\"Layers with all parameters in range: {len(layers_in_range)}\")\nprint(f\"Layers with any parameters out of range: {len(layers_out_of_range)}\")\n\n# Print the layer status report in the order encountered\nprint_layer_inlier_outlier(all_layers)\n\n# Define the output path (adjust this path to a location on your local system)\noutput_path = \"layer_status_plot_stablelm.png\"  # Path to save the plot\n\n# Plot the layer status bar graph and save it\nplot_layer_status(all_layers, output_path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install opencv-python ultralytics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/WongKinYiu/yolov7.git\n!pip install -r yolov7/requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt -P yolov7\n!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt -P yolov7\n!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x.pt -P yolov7\n!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6.pt -P yolov7\n!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6.pt -P yolov7\n!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-d6.pt -P yolov7\n!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt -P yolov7","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## VERIFY IF MODEL IS QUANTIZED FIRST","metadata":{}},{"cell_type":"code","source":"import torch\n\nSF16_MAX = 0.999969482421875\nSF16_MIN = -0.999969482421875\nSF8_MAX = 0.9921875\nSF8_MIN = -0.9921875\n\ndef is_in_sf16_range(tensor):\n    return torch.all((tensor >= SF16_MIN) & (tensor <= SF16_MAX))\n\ndef quantize_to_sf16(tensor):\n    # Clip the tensor to the SF16 range\n    tensor = torch.clamp(tensor, SF16_MIN, SF16_MAX)\n    \n    # Apply the scaling to fit into 15-bit mantissa\n    scaled = (tensor - SF16_MIN) / (SF16_MAX - SF16_MIN) * (2**15 - 1)\n    \n    # Round to nearest integer for mantissa\n    rounded = torch.round(scaled)\n    \n    # Convert back to SF16 range by scaling it again\n    quantized = rounded / (2**15 - 1) * (SF16_MAX - SF16_MIN) + SF16_MIN\n    \n    return quantized\n\ndef apply_selective_sf16_quantization(model):\n    total_params = 0\n    quantized_params = 0\n    memory_usage = 0\n\n    for name, param in model.named_parameters():\n        total_params += param.numel()\n        if is_in_sf16_range(param.data):\n            param.data = quantize_to_sf16(param.data)\n            quantized_params += param.numel()\n            memory_usage += param.numel() * 16 / 8  # 16 bits per parameter\n        else:\n            memory_usage += param.numel() * 32 / 8  # 32 bits for non-quantized parameters\n\n    memory_usage_mb = memory_usage / (1024 * 1024)\n    return total_params, quantized_params, memory_usage_mb\n\ndef verify_selective_sf16_quantization(model):\n    for name, param in model.named_parameters():\n        if is_in_sf16_range(param.data):\n            unique_values = torch.unique(param)\n            if len(unique_values) > 65536:\n                print(f\"Warning: Quantized parameter {name} has more than 65536 unique values\")\n            else:\n                print(f\"Layer {name} is correctly quantized to SF16\")\n        else:\n            print(f\"Layer {name} is not quantized (outside SF16 range)\")\n            \ndef is_in_sf8_range(tensor):\n    return torch.all((tensor >= SF8_MIN) & (tensor <= SF8_MAX))\n\ndef quantize_to_sf8(tensor):\n    # Clip the tensor to the SF8 range\n    tensor = torch.clamp(tensor, SF8_MIN, SF8_MAX)\n    \n    # Apply the scaling to fit into 7-bit mantissa\n    scaled = (tensor - SF8_MIN) / (SF8_MAX - SF8_MIN) * (2**7 - 1)\n    \n    # Round to nearest integer for mantissa\n    rounded = torch.round(scaled)\n    \n    # Convert back to SF8 range by scaling it again\n    quantized = rounded / (2**7 - 1) * (SF8_MAX - SF8_MIN) + SF8_MIN\n    \n    return quantized\n\ndef apply_selective_sf8_quantization(model):\n    total_params = 0\n    quantized_params = 0\n    memory_usage = 0\n\n    for name, param in model.named_parameters():\n        total_params += param.numel()\n        if is_in_sf8_range(param.data):\n            param.data = quantize_to_sf8(param.data)\n            quantized_params += param.numel()\n            memory_usage += param.numel() * 8 / 8  # 8 bits per parameter\n        else:\n            memory_usage += param.numel() * 32 / 8  # 32 bits for non-quantized parameters\n\n    memory_usage_mb = memory_usage / (1024 * 1024)\n    return total_params, quantized_params, memory_usage_mb\n\ndef verify_selective_sf8_quantization(model):\n    for name, param in model.named_parameters():\n        if is_in_sf8_range(param.data):\n            unique_values = torch.unique(param)\n            if len(unique_values) > 255:\n                print(f\"Warning: Quantized parameter {name} has more than 255 unique values\")\n            else:\n                print(f\"Layer {name} is correctly quantized to SF8\")\n        else:\n            print(f\"Layer {name} is not quantized (outside SF8 range)\")\n\n# Usage in your main script\nmodel_name = '/kaggle/working/yolov7/yolov7-d6.pt'\nsf16_model = torch.hub.load('yolov7', 'custom', model_name, source='local', force_reload=True)\nsf16_model.eval()\ntotal_params, quantized_params, quantized_memory = apply_selective_sf16_quantization(sf16_model)\n\nprint(\"SF16 Quantized\")\nprint(f\"Total parameters: {total_params}\")\nprint(f\"Quantized parameters: {quantized_params}\")\nprint(f\"Quantization percentage: {quantized_params / total_params * 100:.2f}%\")\nprint(f\"Estimated memory usage: {quantized_memory:.2f} MB\")\n\nprint(\"\\n\")\n\nsf8_model = torch.hub.load('yolov7', 'custom', model_name, source='local', force_reload=True)\nsf8_model.eval()\ntotal_params, quantized_params, quantized_memory = apply_selective_sf8_quantization(sf8_model)\n\nprint(\"SF8 Quantized\")\nprint(f\"Total parameters: {total_params}\")\nprint(f\"Quantized parameters: {quantized_params}\")\nprint(f\"Quantization percentage: {quantized_params / total_params * 100:.2f}%\")\nprint(f\"Estimated memory usage: {quantized_memory:.2f} MB\")\n\n# Verify quantization\n# print(\"\\nVerifying Selective SF16 Quantization:\")\n# verify_selective_sf16_quantization(quantized_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RUN BENCHMARK COMPARISONS","metadata":{}},{"cell_type":"code","source":"# Download image\n!wget https://upload.wikimedia.org/wikipedia/commons/6/64/Cat_and_dog_standoff_%283926784260%29.jpg","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport cv2\nimport psutil\nimport time\nimport numpy as np\nfrom torchvision.ops import box_iou\n\ndef calculate_memory_fp32(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    memory_usage_fp32 = total_params * 4 / (1024 * 1024)\n    return memory_usage_fp32\n\ndef estimate_complexity(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef detect_image(model, image_path, is_sf16=False, is_sf8=False):\n    \n    process = psutil.Process()\n    initial_memory = process.memory_info().rss\n\n    img = cv2.imread(image_path)\n    img = cv2.resize(img, (640, 640))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    start_time = time.time()\n    results = model(img)\n    end_time = time.time()\n    pred_boxes = results.pandas().xyxy[0]  # Get predictions\n\n    complexity = estimate_complexity(model)\n\n    final_memory = process.memory_info().rss\n    elapsed_time = end_time - start_time\n    memory_used = (final_memory - initial_memory) / (1024 * 1024)\n\n    total_params = sum(p.numel() for p in model.parameters())\n\n    return {\n        \"time\": elapsed_time,\n        \"total_params\": total_params,\n        \"complexity\": complexity,\n        \"pred_boxes\": pred_boxes  # Return predictions\n    }\n\ndef compare_predictions(pred_boxes_normal, pred_boxes_quantized):\n    pred_normal_tensor = torch.tensor(pred_boxes_normal[['xmin', 'ymin', 'xmax', 'ymax']].values, dtype=torch.float32)\n    pred_quantized_tensor = torch.tensor(pred_boxes_quantized[['xmin', 'ymin', 'xmax', 'ymax']].values, dtype=torch.float32)\n\n    # Calculate IoU\n    ious = box_iou(pred_normal_tensor, pred_quantized_tensor)\n    \n    # Calculate mean IoU\n    mean_iou = ious.mean().item() if ious.numel() > 0 else 0\n\n    # Calculate distance between boxes\n    distances = []\n    for normal_box in pred_boxes_normal[['xmin', 'ymin', 'xmax', 'ymax']].values:\n        closest_distance = float('inf')\n        for quantized_box in pred_boxes_quantized[['xmin', 'ymin', 'xmax', 'ymax']].values:\n            distance = np.linalg.norm(normal_box - quantized_box)\n            closest_distance = min(closest_distance, distance)\n        distances.append(closest_distance)\n    \n    mean_distance = np.mean(distances)\n\n    return {\n        \"mean_iou\": mean_iou,\n        \"mean_distance\": mean_distance,\n        \"num_predictions_normal\": len(pred_boxes_normal),\n        \"num_predictions_quantized\": len(pred_boxes_quantized)\n    }\n\n# Path to your image\nimage_path = '/kaggle/working/Cat_and_dog_standoff_(3926784260).jpg'\n\n# Load models\nnormal_model = torch.hub.load('yolov7', 'custom', model_name, source='local', force_reload=True)\nnormal_model.eval()\nnormal_results = detect_image(normal_model, image_path)\nmemory_model = calculate_memory_fp32(model)\nprint(f\"Normal Predictions: {normal_results['pred_boxes']}\")\n\nsf16_model = torch.hub.load('yolov7', 'custom', model_name, source='local', force_reload=True)\nsf16_model.eval()\ntotal_params, sf16_params, sf16_memory = apply_selective_sf16_quantization(sf16_model)\nsf16_results = detect_image(sf16_model, image_path, is_sf16=True)\nprint(f\"SF16 Quantized Predictions: {sf16_results['pred_boxes']}\")\n\n# Compare sf16 predictions\nsf16_comparison_results = compare_predictions(normal_results['pred_boxes'], sf16_results['pred_boxes'])\n\nsf8_model = torch.hub.load('yolov7', 'custom', model_name, source='local', force_reload=True)\nsf8_model.eval()\ntotal_params, sf8_params, sf8_memory = apply_selective_sf8_quantization(sf8_model)\nsf8_results = detect_image(sf8_model, image_path, is_sf8=True)\nprint(f\"SF8 Quantized Predictions: {sf8_results['pred_boxes']}\")\n\n# Compare sf8 predictions\nsf8_comparison_results = compare_predictions(normal_results['pred_boxes'], sf8_results['pred_boxes'])\n\n# Print comparison results\nprint(\"Normal YOLOv7:\")\nprint(f\"Processing time: {normal_results['time']:.2f} seconds\")\nprint(f\"Memory used by model: {memory_model:.2} MB\")\nprint(f\"Estimated complexity: {normal_results['complexity']:,} operations\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(\"\\nSelectively Quantized YOLOv7 (SF16):\")\nprint(f\"Processing time: {sf16_results['time']:.2f} seconds\")\nprint(f\"Memory used by model: {sf16_memory:.2f} MB\")\nprint(f\"Estimated complexity: {sf16_results['complexity']:,} operations\")\nprint(f\"Total parameters: {total_params}\")\nprint(f\"Quantized parameters: {sf16_params}\")\nprint(f\"Percentage of parameters quantized: {(sf16_params / total_params) * 100:.2f}%\")\n\nsf16_time_diff = (normal_results['time'] - sf16_results['time']) / normal_results['time'] * 100\nsf16_memory_diff = (memory_model - sf16_memory) / memory_model * 100\n\nprint(f\"\\nTime improvement: {sf16_time_diff:.2f}%\")\nprint(f\"Memory improvement: {sf16_memory_diff:.2f}%\")\n\nprint(\"Comparison Results:\")\nprint(f\"Mean IoU: {100*sf16_comparison_results['mean_iou']:.2f}%\")\nprint(f\"Mean Distance between predictions: {sf16_comparison_results['mean_distance']:.2f}\")\nprint(f\"Number of predictions (Normal): {sf16_comparison_results['num_predictions_normal']}\")\nprint(f\"Number of predictions (Quantized): {sf16_comparison_results['num_predictions_quantized']}\")\n\nprint(\"\\nSelectively Quantized YOLOv7 (SF8):\")\nprint(f\"Processing time: {sf8_results['time']:.2f} seconds\")\nprint(f\"Memory used by model: {sf8_memory:.2f} MB\")\nprint(f\"Estimated complexity: {sf8_results['complexity']:,} operations\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Quantized parameters: {sf8_params:,}\")\nprint(f\"Percentage of parameters quantized: {(sf8_params / total_params) * 100:.2f}%\")\n\nsf8_time_diff = (normal_results['time'] - sf8_results['time']) / normal_results['time'] * 100\nsf8_memory_diff = (memory_model- sf8_memory) / memory_model * 100\n\nprint(f\"\\nTime improvement: {sf8_time_diff:.2f}%\")\nprint(f\"Memory improvement: {sf8_memory_diff:.2f}%\")\n\nprint(\"Comparison Results:\")\nprint(f\"Mean IoU: {100*sf8_comparison_results['mean_iou']:.2f}%\")\nprint(f\"Mean Distance between predictions: {sf8_comparison_results['mean_distance']:.2f}\")\nprint(f\"Number of predictions (Normal): {sf8_comparison_results['num_predictions_normal']}\")\nprint(f\"Number of predictions (Quantized): {sf8_comparison_results['num_predictions_quantized']}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TESTING ON LARGE LANGUAGE MODELS","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")\n!pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2024-10-16T19:28:09.590721Z","iopub.execute_input":"2024-10-16T19:28:09.591367Z","iopub.status.idle":"2024-10-16T19:28:34.891984Z","shell.execute_reply.started":"2024-10-16T19:28:09.591333Z","shell.execute_reply":"2024-10-16T19:28:34.890927Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nCollecting transformers\n  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nCollecting tokenizers<0.21,>=0.20 (from transformers)\n  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\nSuccessfully installed tokenizers-0.20.1 transformers-4.45.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport time\nimport psutil\n\n# Constants for SF16 and SF8\nSF16_MAX = 0.999969482421875\nSF16_MIN = -0.999969482421875\nSF8_MAX = 0.9921875\nSF8_MIN = -0.9921875\n\ndef is_in_sf16_range(tensor):\n    return torch.all((tensor >= SF16_MIN) & (tensor <= SF16_MAX))\n\ndef is_in_sf8_range(tensor):\n    return torch.all((tensor >= SF8_MIN) & (tensor <= SF8_MAX))\n\ndef quantize_to_sf16(tensor):\n    tensor = torch.clamp(tensor, SF16_MIN, SF16_MAX)\n    scaled = (tensor - SF16_MIN) / (SF16_MAX - SF16_MIN) * (2**15 - 1)\n    rounded = torch.round(scaled)\n    quantized = rounded / (2**15 - 1) * (SF16_MAX - SF16_MIN) + SF16_MIN\n    return quantized\n\ndef quantize_to_sf8(tensor):\n    tensor = torch.clamp(tensor, SF8_MIN, SF8_MAX)\n    scaled = (tensor - SF8_MIN) / (SF8_MAX - SF8_MIN) * (2**7 - 1)\n    rounded = torch.round(scaled)\n    quantized = rounded / (2**7 - 1) * (SF8_MAX - SF8_MIN) + SF8_MIN\n    return quantized\n\ndef apply_selective_quantization(model, quantize_func, is_in_range_func):\n    total_params = 0\n    quantized_params = 0\n    memory_usage = 0\n\n    for name, param in model.named_parameters():\n        total_params += param.numel()\n        if is_in_range_func(param.data):\n            param.data = quantize_func(param.data)\n            quantized_params += param.numel()\n            memory_usage += param.numel() * (16 if quantize_func == quantize_to_sf16 else 8) / 8\n        else:\n            memory_usage += param.numel() * 32 / 8\n\n    memory_usage_mb = memory_usage / (1024 * 1024)\n    return total_params, quantized_params, memory_usage_mb\n\ndef calculate_memory_fp32(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    memory_usage_fp32 = total_params * 4 / (1024 * 1024)\n    return memory_usage_fp32\n\ndef estimate_complexity(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef evaluate_model(model, tokenizer, prompt):\n    process = psutil.Process()\n    initial_memory = process.memory_info().rss\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    start_time = time.time()\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=50)\n    end_time = time.time()\n    \n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    final_memory = process.memory_info().rss\n    elapsed_time = end_time - start_time\n    memory_used = (final_memory - initial_memory) / (1024 * 1024)\n\n    complexity = estimate_complexity(model)\n\n    return {\n        \"time\": elapsed_time,\n        \"memory_used\": memory_used,\n        \"complexity\": complexity,\n        \"generated_text\": generated_text\n    }\n\n# Load models\nmodel_name = \"google/gemma-2-2b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprint(\"Loading normal model...\")\nnormal_model = AutoModelForCausalLM.from_pretrained(model_name)\nnormal_model.eval()\n\nprint(\"Loading and quantizing SF16 model...\")\nsf16_model = AutoModelForCausalLM.from_pretrained(model_name)\nsf16_model.eval()\ntotal_params, sf16_params, sf16_memory = apply_selective_quantization(sf16_model, quantize_to_sf16, is_in_sf16_range)\n\nprint(\"Loading and quantizing SF8 model...\")\nsf8_model = AutoModelForCausalLM.from_pretrained(model_name)\nsf8_model.eval()\n_, sf8_params, sf8_memory = apply_selective_quantization(sf8_model, quantize_to_sf8, is_in_sf8_range)\n\n# Evaluation\nprompt = \"Near them, on the sand, Half sunk a shattered visage lies,\"\n\nprint(\"\\nEvaluating normal model...\")\nnormal_results = evaluate_model(normal_model, tokenizer, prompt)\nmemory_model = calculate_memory_fp32(normal_model)\n\nprint(\"\\nEvaluating SF16 quantized model...\")\nsf16_results = evaluate_model(sf16_model, tokenizer, prompt)\n\nprint(\"\\nEvaluating SF8 quantized model...\")\nsf8_results = evaluate_model(sf8_model, tokenizer, prompt)\n\n# Print results\nprint(\"\\nNormal Gemma-2:\")\nprint(f\"Processing time: {normal_results['time']:.2f} seconds\")\nprint(f\"Memory used by model: {memory_model:.2f} MB\")\nprint(f\"Estimated complexity: {normal_results['complexity']:,} operations\")\nprint(f\"Generated text: {normal_results['generated_text']}\")\n\nprint(\"\\nSF16 Quantized Gemma-2:\")\nprint(f\"Processing time: {sf16_results['time']:.2f} seconds\")\nprint(f\"Memory used by model: {sf16_memory:.2f} MB\")\nprint(f\"Estimated complexity: {sf16_results['complexity']:,} operations\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Quantized parameters: {sf16_params:,}\")\nprint(f\"Percentage of parameters quantized: {(sf16_params / total_params) * 100:.2f}%\")\nprint(f\"Generated text: {sf16_results['generated_text']}\")\n\nsf16_time_diff = (normal_results['time'] - sf16_results['time']) / normal_results['time'] * 100\nsf16_memory_diff = (memory_model - sf16_memory) / memory_model * 100\n\nprint(f\"\\nTime improvement: {sf16_time_diff:.2f}%\")\nprint(f\"Memory improvement: {sf16_memory_diff:.2f}%\")\n\nprint(\"\\nSF8 Gemma-2:\")\nprint(f\"Processing time: {sf8_results['time']:.2f} seconds\")\nprint(f\"Memory used by model: {sf8_memory:.2f} MB\")\nprint(f\"Estimated complexity: {sf8_results['complexity']:,} operations\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Quantized parameters: {sf8_params:,}\")\nprint(f\"Percentage of parameters quantized: {(sf8_params / total_params) * 100:.2f}%\")\nprint(f\"Generated text: {sf8_results['generated_text']}\")\n\nsf8_time_diff = (normal_results['time'] - sf8_results['time']) / normal_results['time'] * 100\nsf8_memory_diff = (memory_model - sf8_memory) / memory_model * 100\n\nprint(f\"\\nTime improvement: {sf8_time_diff:.2f}%\")\nprint(f\"Memory improvement: {sf8_memory_diff:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-10-16T19:28:34.894650Z","iopub.execute_input":"2024-10-16T19:28:34.894970Z","iopub.status.idle":"2024-10-16T19:32:10.013287Z","shell.execute_reply.started":"2024-10-16T19:28:34.894938Z","shell.execute_reply":"2024-10-16T19:32:10.010740Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b12fe333d3cc47e8929186fba7656862"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae56a827c93e4d5986c0f96c195d2145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"837edac9f0524ea3b342c0edde9a31c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7f931241f4849c6a28a9af89d23a853"}},"metadata":{}},{"name":"stdout","text":"Loading normal model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df1a1c6cf5bf4d19a4b848e36c25a9b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45a8bc33950049d68ac652f7c99a9b64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9495469019c948ec8902c3da5ffe2667"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca5694414a8a4e7d888660d51d179afa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa5d143aab474bccb2180bcaaa212b51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d29d8663f6d440089528c3794ef9d8b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acc27bd84aef42d1b1475cd682779294"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f39493ed91164dda9105e42cc98e6db0"}},"metadata":{}},{"name":"stdout","text":"Loading and quantizing SF16 model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bff3146099c1475da326e01ba171a4bc"}},"metadata":{}},{"name":"stdout","text":"Loading and quantizing SF8 model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b6cfcd5b10547c7976854587852d77e"}},"metadata":{}},{"name":"stdout","text":"\nEvaluating normal model...\n","output_type":"stream"},{"name":"stderr","text":"The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluating SF16 quantized model...\n\nEvaluating SF8 quantized model...\n\nNormal Gemma-2:\nProcessing time: 27.07 seconds\nMemory used by model: 9972.92 MB\nEstimated complexity: 2,614,341,888 operations\nGenerated text: Near them, on the sand, Half sunk a shattered visage lies, whose frown, And wrinkled lip, and sneer of cold command, Tell that its sculptor well those passions read Which yet survive, stamped on these lifeless things, The hand that mocked them, and the heart that fed.\n\nThe hand that mocked them,\n\nSF16 Quantized Gemma-2:\nProcessing time: 25.38 seconds\nMemory used by model: 6111.86 MB\nEstimated complexity: 2,614,341,888 operations\nTotal parameters: 2,614,341,888\nQuantized parameters: 2,024,308,224\nPercentage of parameters quantized: 77.43%\nGenerated text: Near them, on the sand, Half sunk a shattered visage lies, whose frown, And wrinkled lip, and sneer of cold command, Tell that its sculptor well those passions read Which yet survive, stamped on these lifeless things, The hand that mocked them, and the heart that fed.\n\nThe hand that mocked them,\n\nTime improvement: 6.25%\nMemory improvement: 38.72%\n\nSF8 Gemma-2:\nProcessing time: 24.57 seconds\nMemory used by model: 4181.33 MB\nEstimated complexity: 2,614,341,888 operations\nTotal parameters: 2,614,341,888\nQuantized parameters: 2,024,308,224\nPercentage of parameters quantized: 77.43%\nGenerated text: Near them, on the sand, Half sunk a shattered visage lies, UST->___x kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan\n\nTime improvement: 9.23%\nMemory improvement: 58.07%\n","output_type":"stream"}]}]}